---
title: Learning Nash Equilibria via Entropy-Regulated Policy Approximations
publication_types:
  - "1"
authors:
  - Yue Guan
  - Qifan Zhang
  - Panagiotis Tsiotras
author_notes:
  - Equal Contribution
  - Equal Contribution
publication: In *International Joint Conference on Artificial Intelligence*
publication_short: In *IJCAI-21*
abstract: >-
  We explore the use of policy approximations to reduce the computational cost
  of learning Nash equilibria in zero-sum stochastic games. We propose a new
  Q-learning type algorithm that uses a sequence of entropy-regularized soft
  policies to approximate the Nash policy during the Q-function updates. We
  prove that under certain conditions, by updating the entropy regularization,
  the algorithm converges to a Nash equilibrium. 

  We also demonstrate the proposed algorithm's ability to transfer previous training experiences, enabling the agents to adapt quickly to new environments. We provide a dynamic hyper-parameter schedule scheme to further expedite convergence. Empirical results applied to a number of stochastic games verify that the proposed algorithm converges to a Nash equilibrium, while exhibiting a major speed-up over existing algorithms.
draft: false
featured: true
image:
  filename: featured
  focal_point: Smart
  preview_only: false
summary: >-
  In this work, we use entropy-regulated policy approximations expedite learning
  Nash equilibrium in zero-sum stochastic games. 


  Accepted at IJCAI-21 (**13.9% acceptance rate!**).
date: 2021-06-18T22:00:46.161Z
---
